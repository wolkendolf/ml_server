{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск сервера:\n",
    "```bash\n",
    "uv run --active uvicorn server.main:app --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "Проверка статуса сервера:\n",
    "```bash\n",
    "curl http://localhost:8000/status\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Демонстрация работы с ML-сервером\n",
    "\n",
    "Посмотрим на работу клиентской части для взаимодействия с ML-сервером. Не забудьте установить все необходимые зависимости проекта.\n",
    "Демонстрируются следующие функции:\n",
    "- Синхронное обучение двух моделей (длительность ≥60 секунд для каждой).\n",
    "- Асинхронное обучение двух моделей с ускорением.\n",
    "- Асинхронный вызов нескольких предсказаний.\n",
    "- Загрузка, выгрузка, удаление моделей и удаление всех моделей.\n",
    "\n",
    "Есть обработка ошибок сервера (404, 429, 500) и сетевых исключений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Базовый URL сервера\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Генерация синтетического датасета для обучения (длительность ≥60 секунд)\n",
    "X, y = make_classification(\n",
    "    n_samples=20000,  # Большое число объектов\n",
    "    n_features=200,   # Большое число признаков\n",
    "    n_classes=3,\n",
    "    n_informative=5,\n",
    "    random_state=42\n",
    ")\n",
    "X = X.tolist()\n",
    "y = y.tolist()\n",
    "\n",
    "# Данные для предсказания\n",
    "X_pred = X[:100]  # Первые 10 объектов\n",
    "\n",
    "# Конфигурации моделей\n",
    "models = [\n",
    "    {\"name\": \"logreg_model\", \"type\": \"logistic_regression\", \"params\": {}},\n",
    "    {\"name\": \"gb_model\", \"type\": \"gradient_boosting\", \"params\": {\"n_estimators\": 2000}}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Синхронное обучение двух моделей\n",
    "\n",
    "**Ожидаемое действие**: Обучаем две модели (`logistic_regression` и `gradient_boosting`) последовательно с помощью `requests`. Для `gradient_boosting` задаём `n_estimators=1000`, чтобы обучение длилось ≥60 секунд. Измеряем общее время. Обрабатываем возможные ошибки (например, отсутствие свободных ядер или превышение лимита моделей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting synchronous training...\n",
      "logreg_model: Training started for model_name 'logreg_model' (took 5.82s)\n",
      "gb_model: Training started for model_name 'gb_model' (took 6.22s)\n",
      "Total synchronous training time: 12.07s\n"
     ]
    }
   ],
   "source": [
    "def train_sync(model_name: str, model_type: str, model_params: dict):\n",
    "    \"\"\"Синхронный вызов обучения модели.\"\"\"\n",
    "    payload = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"config\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_type\": model_type,\n",
    "            \"model_params\": model_params\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(f\"{BASE_URL}/fit\", json=payload, timeout=300)\n",
    "        response.raise_for_status()\n",
    "        print(f\"{model_name}: {response.json()['message']} (took {time.time() - start_time:.2f}s)\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 429:\n",
    "            print(f\"Error for {model_name}: No available cores or max models reached: {response.json()['detail']}\")\n",
    "        elif response.status_code == 500:\n",
    "            print(f\"Error for {model_name}: Server error: {response.json()['detail']}\")\n",
    "        else:\n",
    "            print(f\"Error for {model_name}: {str(e)}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error for {model_name}: {str(e)}\")\n",
    "\n",
    "# Синхронное обучение\n",
    "print(\"Starting synchronous training...\")\n",
    "sync_start = time.time()\n",
    "for model in models:\n",
    "    train_sync(model[\"name\"], model[\"type\"], model[\"params\"])\n",
    "sync_duration = time.time() - sync_start\n",
    "print(f\"Total synchronous training time: {sync_duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Асинхронное обучение двух моделей\n",
    "\n",
    "Теперь обучаем те же две модели **асинхронно**. Видим, что обучение выполняется быстрее, чем синхронное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting asynchronous training...\n",
      "gb_model: Training started for model_name 'gb_model' (took 4.19s)\n",
      "logreg_model: Training started for model_name 'logreg_model' (took 7.96s)\n",
      "Total asynchronous training time: 7.97s\n",
      "Speedup: 1.09x\n"
     ]
    }
   ],
   "source": [
    "async def train_async(session: aiohttp.ClientSession, model_name: str, model_type: str, model_params: dict):\n",
    "    \"\"\"Асинхронный вызов обучения модели.\"\"\"\n",
    "    payload = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"config\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_type\": model_type,\n",
    "            \"model_params\": model_params\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        async with session.post(f\"{BASE_URL}/fit\", json=payload, timeout=300) as response:\n",
    "            data = await response.json()\n",
    "        \n",
    "            if response.status == 200:\n",
    "                print(f\"{model_name}: Training started\")\n",
    "                \n",
    "                # Периодически проверяем статус\n",
    "                while True:\n",
    "                    await asyncio.sleep(5)\n",
    "                    async with session.get(f\"{BASE_URL}/status\") as status_resp:\n",
    "                        status = await status_resp.json()\n",
    "                        if model_name in status['available_models']:\n",
    "                            duration = time.time() - start_time\n",
    "                            print(f\"{model_name}: Training completed (took {duration:.2f}s)\")\n",
    "                            break\n",
    "            elif response.status == 429:\n",
    "                print(f\"Error for {model_name}: No available cores or max models reached: {await response.json()['detail']}\")\n",
    "            elif response.status == 500:\n",
    "                print(f\"Error for {model_name}: Server error: {await response.json()['detail']}\")\n",
    "            else:\n",
    "                print(f\"Error for {model_name}: HTTP {response.status}\")\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Network error for {model_name}: {str(e)}\")\n",
    "\n",
    "async def async_training():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [\n",
    "            train_async(session, model[\"name\"], model[\"type\"], model[\"params\"])\n",
    "            for model in models\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "# Асинхронное обучение\n",
    "print(\"Starting asynchronous training...\")\n",
    "async_start = time.time()\n",
    "await async_training()\n",
    "async_duration = time.time() - async_start\n",
    "print(f\"Total asynchronous training time: {async_duration:.2f}s\")\n",
    "print(f\"Speedup: {sync_duration / async_duration:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def wait_for_training_completion(session: aiohttp.ClientSession, model_name: str, timeout: int = 300):\n",
    "    \"\"\"Ожидает завершения обучения модели\"\"\"\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        # Проверяем статус сервера\n",
    "        async with session.get(f\"{BASE_URL}/status\") as resp:\n",
    "            status = await resp.json()\n",
    "            available_models = list(Path(MODEL_DIR).glob(\"*.pkl\"))\n",
    "            model_files = [m.stem for m in available_models]\n",
    "            \n",
    "            if model_name in model_files:\n",
    "                return True\n",
    "            \n",
    "        await asyncio.sleep(5)\n",
    "    \n",
    "    raise TimeoutError(f\"Training for {model_name} didn't complete in {timeout} seconds\")\n",
    "\n",
    "async def load_model(session: aiohttp.ClientSession, model_name: str):\n",
    "    \"\"\"Загружает модель в память сервера\"\"\"\n",
    "    payload = {\"config\": {\"model_name\": model_name}}\n",
    "    async with session.post(f\"{BASE_URL}/load\", json=payload) as resp:\n",
    "        if resp.status == 200:\n",
    "            return True\n",
    "        error = await resp.json()\n",
    "        raise ValueError(f\"Failed to load {model_name}: {error.get('detail')}\")\n",
    "\n",
    "async def async_training_and_predict():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Запускаем обучение\n",
    "        train_tasks = [\n",
    "            train_async(session, model[\"name\"], model[\"type\"], model[\"params\"])\n",
    "            for model in models\n",
    "        ]\n",
    "        await asyncio.gather(*train_tasks)\n",
    "        \n",
    "        # Ожидаем завершения обучения\n",
    "        print(\"Waiting for training to complete...\")\n",
    "        await asyncio.gather(*[\n",
    "            wait_for_training_completion(session, model[\"name\"])\n",
    "            for model in models\n",
    "        ])\n",
    "        \n",
    "        # Загружаем модели\n",
    "        print(\"Loading models...\")\n",
    "        await asyncio.gather(*[\n",
    "            load_model(session, model[\"name\"])\n",
    "            for model in models\n",
    "        ])\n",
    "        \n",
    "        # Теперь делаем предсказания\n",
    "        print(\"Starting predictions...\")\n",
    "        await async_predictions()\n",
    "\n",
    "# Запуск всего pipeline\n",
    "print(\"Starting full training and prediction pipeline...\")\n",
    "start_time = time.time()\n",
    "await async_training_and_predict()\n",
    "print(f\"Total time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Асинхронный вызов нескольких предсказаний\n",
    "\n",
    "Сделаем асинхронные предсказания для двух моделей одновременно. Используем небольшой набор данных (`X_pred`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting asynchronous predictions...\n",
      "logreg_model: Predictions received: [1, 1, 2, 1, 1]...\n",
      "Error for gb_model: Model not found: Model not found: Model 'gb_model' not found on disk\n"
     ]
    }
   ],
   "source": [
    "async def predict_async(session: aiohttp.ClientSession, model_name: str):\n",
    "    \"\"\"Асинхронный вызов предсказания.\"\"\"\n",
    "    payload = {\n",
    "        \"y\": X_pred,\n",
    "        \"config\": {\"model_name\": model_name}\n",
    "    }\n",
    "    try:\n",
    "        async with session.post(f\"{BASE_URL}/predict\", json=payload, timeout=10) as response:\n",
    "            if response.status == 200:\n",
    "                data = await response.json()\n",
    "                print(f\"{model_name}: Predictions received: {data['predictions'][:5]}...\")\n",
    "            else:\n",
    "                try:\n",
    "                    error_data = await response.json()\n",
    "                    error_detail = error_data.get('detail', 'No detail provided')\n",
    "                except (aiohttp.ContentTypeError, ValueError):\n",
    "                    error_detail = f\"HTTP {response.status}\"\n",
    " \n",
    "                if response.status == 404:\n",
    "                    print(f\"Error for {model_name}: Model not found: {error_detail}\")\n",
    "                elif response.status == 429:\n",
    "                    print(f\"Error for {model_name}: Too many requests: {error_detail}\")\n",
    "                elif response.status == 500:\n",
    "                    print(f\"Error for {model_name}: Server error: {error_detail}\")\n",
    "                else:\n",
    "                    print(f\"Error for {model_name}: {error_detail}\")\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Network error for {model_name}: {str(e)}\")\n",
    "\n",
    "async def async_predictions():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [\n",
    "            predict_async(session, model[\"name\"])\n",
    "            for model in models\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "# Асинхронные предсказания\n",
    "print(\"Starting asynchronous predictions...\")\n",
    "await async_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Демонстрация остальных функций сервера\n",
    "\n",
    "Поделаем разные операции с моделями: загрузку, выгрузку, удаление одной модели и удаление всех моделей. Используем синхронные вызовы через `requests` для простоты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: Model 'logreg_model' loaded successfully.\n",
      "unload: Model 'logreg_model' unloaded successfully.\n",
      "remove: Model 'logreg_model' removed successfully.\n",
      "remove_all: All models removed successfully.\n"
     ]
    }
   ],
   "source": [
    "def manage_model(endpoint: str, model_name: str = None):\n",
    "    \"\"\"Универсальная функция для вызова управляющих эндпоинтов.\"\"\"\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    payload = {\"config\": {\"model_name\": model_name}} if model_name else {}\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        print(f\"{endpoint}: {response.json()['message']}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 404:\n",
    "            print(f\"Error for {endpoint}: Model not found: {response.json()['detail']}\")\n",
    "        elif response.status_code == 500:\n",
    "            print(f\"Error for {endpoint}: Server error: {response.json()['detail']}\")\n",
    "        else:\n",
    "            print(f\"Error for {endpoint}: {str(e)}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error for {endpoint}: {str(e)}\")\n",
    "\n",
    "# Загрузка модели\n",
    "manage_model(\"load\", \"logreg_model\")\n",
    "\n",
    "# Выгрузка модели\n",
    "manage_model(\"unload\", \"logreg_model\")\n",
    "\n",
    "# Удаление модели\n",
    "manage_model(\"remove\", \"logreg_model\")\n",
    "\n",
    "# Удаление всех моделей\n",
    "manage_model(\"remove_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Проверка статуса сервера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server status: {\n",
      "  \"cpu_cores\": 8,\n",
      "  \"max_processes\": 7,\n",
      "  \"active_processes\": 2,\n",
      "  \"active_requests\": 0,\n",
      "  \"max_requests\": 16,\n",
      "  \"model_dir\": \"/app/server/storage\",\n",
      "  \"max_models\": 10\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/status\", timeout=10)\n",
    "    response.raise_for_status()\n",
    "    print(\"Server status:\", json.dumps(response.json(), indent=2))\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Network error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
