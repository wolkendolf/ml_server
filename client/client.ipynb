{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск сервера:\n",
    "```bash\n",
    "uv run --active uvicorn server.main:app --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "Проверка статуса сервера:\n",
    "```bash\n",
    "curl http://localhost:8000/status\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Демонстрация работы с ML-сервером\n",
    "\n",
    "Посмотрим на работу клиентской части для взаимодействия с ML-сервером. Не забудьте установить все необходимые зависимости проекта.\n",
    "Демонстрируются следующие функции:\n",
    "- Синхронное обучение двух моделей (длительность ≥60 секунд для каждой).\n",
    "- Асинхронное обучение двух моделей с ускорением.\n",
    "- Асинхронный вызов нескольких предсказаний.\n",
    "- Загрузка, выгрузка, удаление моделей и удаление всех моделей.\n",
    "\n",
    "Есть обработка ошибок сервера (404, 429, 500) и сетевых исключений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Базовый URL сервера\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "MODEL_DIR = f\"{Path.cwd().parent}/server/storage\"\n",
    "\n",
    "# Генерация синтетического датасета для обучения (длительность ≥60 секунд)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,  # Большое число объектов\n",
    "    n_features=100,   # Большое число признаков\n",
    "    n_classes=2,\n",
    "    n_informative=5,\n",
    "    random_state=42\n",
    ")\n",
    "X = X.tolist()\n",
    "y = y.tolist()\n",
    "\n",
    "# Данные для предсказания\n",
    "X_pred = X[:10]  # Первые 10 объектов\n",
    "\n",
    "# Конфигурации моделей\n",
    "models = [\n",
    "    {\"name\": \"logreg_model\", \"type\": \"logistic_regression\", \"params\": {}},\n",
    "    {\"name\": \"gb_model\", \"type\": \"gradient_boosting\", \"params\": {\"n_estimators\": 2000}}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Синхронное обучение двух моделей\n",
    "\n",
    "**Ожидаемое действие**: Обучаем две модели (`logistic_regression` и `gradient_boosting`) последовательно с помощью `requests`. Для `gradient_boosting` задаём `n_estimators=2000`, чтобы обучение длилось ≥60 секунд. Измеряем общее время. Обрабатываем возможные ошибки (например, отсутствие свободных ядер или превышение лимита моделей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server status before training:\n",
      "{\n",
      "  \"cpu_cores\": 4,\n",
      "  \"max_processes\": 3,\n",
      "  \"active_processes\": -1,\n",
      "  \"active_requests\": 0,\n",
      "  \"max_requests\": 8,\n",
      "  \"model_dir\": \"./server/storage\",\n",
      "  \"max_models\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Сначала очистим сервер от моделей\n",
    "try:\n",
    "    requests.post(f\"{BASE_URL}/remove_all\", timeout=10)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error clearing models: {str(e)}\")\n",
    "\n",
    "print(\"Server status before training:\")\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/status\", timeout=10)\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error getting status: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало синхронного обучения...\n",
      "logreg_model: Обучение завершено за 2.2с\n",
      "gb_model: Обучение завершено за 64.2с\n",
      "Total synchronous training time: 66.41s\n"
     ]
    }
   ],
   "source": [
    "def train_sync(model: dict):\n",
    "    \"\"\"Синхронный вызов обучения модели.\"\"\"\n",
    "    payload = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"config\": {\n",
    "            \"model_name\": model[\"name\"],\n",
    "            \"model_type\": model[\"type\"],\n",
    "            \"model_params\": model[\"params\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start = time.time()\n",
    "        response = requests.post(f\"{BASE_URL}/fit\", json=payload, timeout=300)\n",
    "        \n",
    "        timeout = 600\n",
    "        while True:\n",
    "            model_path = Path(MODEL_DIR) / f\"{model['name']}.pkl\"\n",
    "            if model_path.exists():\n",
    "                break\n",
    "            if time.time() - start > timeout:\n",
    "                raise TimeoutError(f\"Model {model['name']} not saved after {timeout}s.\")\n",
    "            time.sleep(2)\n",
    "        time_exec = time.time() - start\n",
    "\n",
    "        response.raise_for_status()\n",
    "            \n",
    "        print(f\"{model['name']}: Обучение завершено за {time_exec:.1f}с\")\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 429:\n",
    "            print(f\"Error for {model['name']}: No available cores or max models reached: {response.json()['detail']}\")\n",
    "        elif response.status_code == 500:\n",
    "            print(f\"Error for {model['name']}: Server error: {response.json()['detail']}\")\n",
    "        else:\n",
    "            print(f\"Error for {model['name']}: {str(e)}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error for {model['name']}: {str(e)}\")\n",
    "\n",
    "# Синхронное обучение\n",
    "print(\"Начало синхронного обучения...\")\n",
    "sync_start = time.time()\n",
    "for model in models:\n",
    "    train_sync(model)\n",
    "sync_duration = time.time() - sync_start\n",
    "print(f\"Total synchronous training time: {sync_duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Асинхронное обучение двух моделей\n",
    "\n",
    "Теперь обучаем те же две модели **асинхронно**. Видим, что обучение выполняется быстрее, чем синхронное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server status before training:\n",
      "{\n",
      "  \"cpu_cores\": 4,\n",
      "  \"max_processes\": 3,\n",
      "  \"active_processes\": -1,\n",
      "  \"active_requests\": 0,\n",
      "  \"max_requests\": 8,\n",
      "  \"model_dir\": \"./server/storage\",\n",
      "  \"max_models\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Сначала очистим сервер от моделей\n",
    "try:\n",
    "    requests.post(f\"{BASE_URL}/remove_all\", timeout=10)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error clearing models: {str(e)}\")\n",
    "\n",
    "print(\"Server status before training:\")\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/status\", timeout=10)\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error getting status: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7d8d47e8a950>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало асинхронного обучения...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7d8d47e8b910>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg_model: Обучение завершено за 2.3с\n",
      "gb_model: Обучение завершено за 64.3с\n",
      "Total asynchronous training time: 64.42s\n",
      "Speedup: 1.03x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "# import asyncio\n",
    "import aiohttp\n",
    "\n",
    "# убираем ошибку \"RuntimeError: This event loop is already running\"\n",
    "import nest_asyncio\n",
    "\n",
    "async def train_async(session: aiohttp.ClientSession, model: dict):\n",
    "    \"\"\"Асинхронный вызов обучения модели.\"\"\"\n",
    "    payload = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"config\": {\n",
    "            \"model_name\": model[\"name\"],\n",
    "            \"model_type\": model[\"type\"],\n",
    "            \"model_params\": model[\"params\"]\n",
    "        }\n",
    "    }\n",
    "    timeout = 600\n",
    "    try:\n",
    "        start = time.time()\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(f\"{BASE_URL}/fit\", json=payload, timeout=300) as response:\n",
    "                resp_json = await response.json()\n",
    "                await asyncio.sleep(0)  # yield control\n",
    "        # Ожидание сохранения модели как и в sync-версии\n",
    "        while True:\n",
    "            model_path = Path(MODEL_DIR) / f\"{model['name']}.pkl\"\n",
    "            if model_path.exists():\n",
    "                break\n",
    "            if time.time() - start > timeout:\n",
    "                raise TimeoutError(f\"Model {model['name']} not saved after {timeout}s.\")\n",
    "            await asyncio.sleep(2)\n",
    "        time_exec = time.time() - start\n",
    "        if response.status != 200:\n",
    "            if response.status == 429:\n",
    "                print(f\"Error for {model['name']}: No available cores or max models reached: {resp_json.get('detail')}\")\n",
    "            elif response.status == 500:\n",
    "                print(f\"Error for {model['name']}: Server error: {resp_json.get('detail')}\")\n",
    "            else:\n",
    "                print(f\"Error for {model['name']}: {response.reason}\")\n",
    "        else:\n",
    "            print(f\"{model['name']}: Обучение завершено за {time_exec:.1f}с\")\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Timeout error for {model['name']}: Model not saved in time.\")\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Network error for {model['name']}: {str(e)}\")\n",
    "\n",
    "async def main():\n",
    "    print(\"Начало асинхронного обучения...\")\n",
    "    async_start = time.time()\n",
    "    tasks = [train_async(aiohttp.ClientSession(), model) for model in models]\n",
    "    await asyncio.gather(*tasks)\n",
    "    async_duration = time.time() - async_start\n",
    "    print(f\"Total asynchronous training time: {async_duration:.2f}s\")\n",
    "    print(f\"Speedup: {sync_duration / async_duration:.2f}x\")\n",
    "    \n",
    "\n",
    "nest_asyncio.apply()\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Асинхронный вызов нескольких предсказаний\n",
    "\n",
    "Сделаем асинхронные предсказания для двух моделей одновременно. Используем небольшой набор данных (`X_pred`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting asynchronous predictions...\n",
      "gb_model: Predictions received: [0, 1, 1, 1, 1]...\n",
      "logreg_model: Predictions received: [0, 1, 0, 1, 0]...\n"
     ]
    }
   ],
   "source": [
    "async def predict_async(session: aiohttp.ClientSession, model_name: str):\n",
    "    \"\"\"Асинхронный вызов предсказания.\"\"\"\n",
    "    payload = {\n",
    "        \"y\": X_pred,\n",
    "        \"config\": {\"model_name\": model_name}\n",
    "    }\n",
    "    try:\n",
    "        async with session.post(f\"{BASE_URL}/predict\", json=payload, timeout=10) as response:\n",
    "            if response.status == 200:\n",
    "                data = await response.json()\n",
    "                print(f\"{model_name}: Predictions received: {data['predictions'][:5]}...\")\n",
    "            else:\n",
    "                try:\n",
    "                    error_data = await response.json()\n",
    "                    error_detail = error_data.get('detail', 'No detail provided')\n",
    "                except (aiohttp.ContentTypeError, ValueError):\n",
    "                    error_detail = f\"HTTP {response.status}\"\n",
    " \n",
    "                if response.status == 404:\n",
    "                    print(f\"Error for {model_name}: Model not found: {error_detail}\")\n",
    "                elif response.status == 429:\n",
    "                    print(f\"Error for {model_name}: Too many requests: {error_detail}\")\n",
    "                elif response.status == 500:\n",
    "                    print(f\"Error for {model_name}: Server error: {error_detail}\")\n",
    "                else:\n",
    "                    print(f\"Error for {model_name}: {error_detail}\")\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Network error for {model_name}: {str(e)}\")\n",
    "\n",
    "async def async_predictions():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [\n",
    "            predict_async(session, model[\"name\"])\n",
    "            for model in models\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "# Асинхронные предсказания\n",
    "print(\"Starting asynchronous predictions...\")\n",
    "await async_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Демонстрация остальных функций сервера\n",
    "\n",
    "Поделаем разные операции с моделями: загрузку, выгрузку, удаление одной модели и удаление всех моделей. Используем синхронные вызовы через `requests` для простоты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: Model 'logreg_model' loaded successfully.\n",
      "unload: Model 'logreg_model' unloaded successfully.\n",
      "remove: Model 'logreg_model' removed successfully.\n",
      "remove_all: All models removed successfully.\n"
     ]
    }
   ],
   "source": [
    "def manage_model(endpoint: str, model_name: str = None):\n",
    "    \"\"\"Универсальная функция для вызова управляющих эндпоинтов.\"\"\"\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    payload = {\"config\": {\"model_name\": model_name}} if model_name else {}\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        print(f\"{endpoint}: {response.json()['message']}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if response.status_code == 404:\n",
    "            print(f\"Error for {endpoint}: Model not found: {response.json()['detail']}\")\n",
    "        elif response.status_code == 500:\n",
    "            print(f\"Error for {endpoint}: Server error: {response.json()['detail']}\")\n",
    "        else:\n",
    "            print(f\"Error for {endpoint}: {str(e)}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error for {endpoint}: {str(e)}\")\n",
    "\n",
    "# Загрузка модели\n",
    "manage_model(\"load\", \"logreg_model\")\n",
    "\n",
    "# Выгрузка модели\n",
    "manage_model(\"unload\", \"logreg_model\")\n",
    "\n",
    "# Удаление модели\n",
    "manage_model(\"remove\", \"logreg_model\")\n",
    "\n",
    "# Удаление всех моделей\n",
    "manage_model(\"remove_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Проверка статуса сервера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server status: {\n",
      "  \"cpu_cores\": 4,\n",
      "  \"max_processes\": 3,\n",
      "  \"active_processes\": 0,\n",
      "  \"active_requests\": 0,\n",
      "  \"max_requests\": 8,\n",
      "  \"model_dir\": \"./server/storage\",\n",
      "  \"max_models\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/status\", timeout=10)\n",
    "    response.raise_for_status()\n",
    "    print(\"Server status:\", json.dumps(response.json(), indent=2))\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Network error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
